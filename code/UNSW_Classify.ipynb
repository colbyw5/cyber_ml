{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option(\"display.colheader_justify\", \"center\")\n",
    "\n",
    "# EDA Packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# ML Packages\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will implement and evaluate 4 classification algorithms for the UNSW dataset.  The label is binary 1: attack 0: normal.  I will also try some hyperparameter tuning, though I am limited by the computational complexity needed for cross validation on my laptop.  The 4 algorithms are:\n",
    "\n",
    "* Random Forest\n",
    "* Naive Bayes\n",
    "* SVM with a linear kernel\n",
    "* Perceptron\n",
    "\n",
    "I found a quick intro on some common ML algorithms, including a few we use below: https://towardsdatascience.com/an-introduction-to-nine-essential-machine-learning-algorithms-ee0efbb61e0\n",
    "\n",
    "The link above does not include the perceptron algorithm, but here is a quick intro: https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53\n",
    "\n",
    "There are many algorithms we can try, and as we better understand the data we will gain understanding of the pros/cons of different classification methods.  I also plan on trying a shallow neural network approach soon.\n",
    "\n",
    "Loading UNSW training set below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw_train = pd.read_csv('https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv')\n",
    "\n",
    "unsw_train[unsw_train.select_dtypes('object').columns] = unsw_train.select_dtypes('object').apply(lambda x: x.astype('category'))\n",
    "unsw_train['label'] = unsw_train['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.121478</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>258</td>\n",
       "      <td>172</td>\n",
       "      <td>74.087490</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.649902</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>734</td>\n",
       "      <td>42014</td>\n",
       "      <td>78.473372</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.623129</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>364</td>\n",
       "      <td>13186</td>\n",
       "      <td>14.170161</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.681642</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>628</td>\n",
       "      <td>770</td>\n",
       "      <td>13.677108</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.449454</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>534</td>\n",
       "      <td>268</td>\n",
       "      <td>33.373826</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     dur   proto service state  spkts  dpkts  sbytes  dbytes    rate     \\\n",
       "0   1  0.121478   tcp      -    FIN    6      4      258      172  74.087490   \n",
       "1   2  0.649902   tcp      -    FIN   14     38      734    42014  78.473372   \n",
       "2   3  1.623129   tcp      -    FIN    8     16      364    13186  14.170161   \n",
       "3   4  1.681642   tcp    ftp    FIN   12     12      628      770  13.677108   \n",
       "4   5  0.449454   tcp      -    FIN   10      6      534      268  33.373826   \n",
       "\n",
       "   ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  \\\n",
       "0  ...          1                1              0            0       \n",
       "1  ...          1                2              0            0       \n",
       "2  ...          1                3              0            0       \n",
       "3  ...          1                3              1            1       \n",
       "4  ...          1               40              0            0       \n",
       "\n",
       "   ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  attack_cat  \\\n",
       "0          0              1           1             0           Normal     \n",
       "1          0              1           6             0           Normal     \n",
       "2          0              2           6             0           Normal     \n",
       "3          0              2           1             0           Normal     \n",
       "4          0              2          39             0           Normal     \n",
       "\n",
       "   label  \n",
       "0    0    \n",
       "1    0    \n",
       "2    0    \n",
       "3    0    \n",
       "4    0    \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsw_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the test data for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw_test = pd.read_csv('https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "unsw_test['label'] = unsw_test['label'].astype('category')\n",
    "unsw_test[unsw_test.select_dtypes('object').columns] = unsw_test.select_dtypes('object').apply(lambda x: x.astype('category'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First getting idea of the frequency of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.680622\n",
       "0    0.319378\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsw_train['label'].value_counts() / len(unsw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((sum(np.random.randint(0,2,len(unsw_train)) == unsw_train['label']) / len(unsw_train)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By random guess, we would identify about 50% of attacks.  Lets compare this to random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most scikit-learn classifers need categorical variables to be one-hot endcoded.  This means a feature with three possible categories (e.g. small, medium large) would be transformed into 3 columns, with a 1 in the row of the corresponding value and 0's in the other cells.  For example, small would be row 100, medium 010, etc.\n",
    "\n",
    "This is easily implemented in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>...</th>\n",
       "      <th>attack_cat_DoS</th>\n",
       "      <th>attack_cat_Exploits</th>\n",
       "      <th>attack_cat_Fuzzers</th>\n",
       "      <th>attack_cat_Generic</th>\n",
       "      <th>attack_cat_Normal</th>\n",
       "      <th>attack_cat_Reconnaissance</th>\n",
       "      <th>attack_cat_Shellcode</th>\n",
       "      <th>attack_cat_Worms</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.121478</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>258</td>\n",
       "      <td>172</td>\n",
       "      <td>74.087490</td>\n",
       "      <td>252</td>\n",
       "      <td>254</td>\n",
       "      <td>14158.942380</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.649902</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>734</td>\n",
       "      <td>42014</td>\n",
       "      <td>78.473372</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>8395.112305</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.623129</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>364</td>\n",
       "      <td>13186</td>\n",
       "      <td>14.170161</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>1572.271851</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.681642</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>628</td>\n",
       "      <td>770</td>\n",
       "      <td>13.677108</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>2740.178955</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.449454</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>534</td>\n",
       "      <td>268</td>\n",
       "      <td>33.373826</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>8561.499023</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     dur    spkts  dpkts  sbytes  dbytes    rate     sttl  dttl  \\\n",
       "0   1  0.121478    6      4      258      172  74.087490   252   254   \n",
       "1   2  0.649902   14     38      734    42014  78.473372    62   252   \n",
       "2   3  1.623129    8     16      364    13186  14.170161    62   252   \n",
       "3   4  1.681642   12     12      628      770  13.677108    62   252   \n",
       "4   5  0.449454   10      6      534      268  33.373826   254   252   \n",
       "\n",
       "       sload     ...  attack_cat_DoS  attack_cat_Exploits  attack_cat_Fuzzers  \\\n",
       "0  14158.942380  ...         0                 0                    0           \n",
       "1   8395.112305  ...         0                 0                    0           \n",
       "2   1572.271851  ...         0                 0                    0           \n",
       "3   2740.178955  ...         0                 0                    0           \n",
       "4   8561.499023  ...         0                 0                    0           \n",
       "\n",
       "   attack_cat_Generic  attack_cat_Normal  attack_cat_Reconnaissance  \\\n",
       "0           0                  1                      0               \n",
       "1           0                  1                      0               \n",
       "2           0                  1                      0               \n",
       "3           0                  1                      0               \n",
       "4           0                  1                      0               \n",
       "\n",
       "   attack_cat_Shellcode  attack_cat_Worms  label_0  label_1  \n",
       "0            0                   0            1        0     \n",
       "1            0                   0            1        0     \n",
       "2            0                   0            1        0     \n",
       "3            0                   0            1        0     \n",
       "4            0                   0            1        0     \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(unsw_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this extends our feature dimensionality from less than 50 to over 200, we will stick with float/into features only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 5-fold cross validation to tune the following hyperparameters: number of estimators, tree depth, number of features considered at each split.  Instead of trying all combinations of these hyperparameters, we will conduct a random search over 20 different combinations.  This is not best practice, and its likely we will miss the optimal hyperparameters.  However, we are limited by the speed of our laptops (ASECC will be a huge help!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'max_features': [2, 4, 8, 10, 12],\n",
    "    'n_estimators': [50, 100, 300, 500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(estimator = random_forest, param_distributions = param_grid, \n",
    "                          cv = 5, verbose = 1, n_iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the best performing parameters from the Random Search to train a RF classifier and evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "random_search.fit(unsw_train.select_dtypes(['float', 'int']).drop('id', axis=1), unsw_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_1 = RandomForestClassifier(max_depth=11, max_features=4, n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=11, max_features=4,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_1.fit(unsw_train.select_dtypes(['float', 'int']).drop('id', axis=1), unsw_train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predict = random_forest_1.predict(unsw_test.select_dtypes(['float', 'int']).drop('id', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.66      0.79     37000\n",
      "           1       0.78      0.99      0.88     45332\n",
      "\n",
      "    accuracy                           0.85     82332\n",
      "   macro avg       0.89      0.83      0.84     82332\n",
      "weighted avg       0.88      0.85      0.84     82332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(unsw_test['label'], rf_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier\n",
    "\n",
    "Because the features of this dataset are on different scales, it is important that we do the follwing to each feature: 1) center (subtract the mean) and 2) scale (divide by the standard deviation).  SVM classifiers are optimized using some distance metric, meaning we can only use the continuous features (no categorical features) and different scales will hinder model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= StandardScaler()\n",
    "scaled_train = sc.fit_transform(unsw_train.select_dtypes(['float', 'int']).drop('id', axis=1))\n",
    "scaled_test = sc.transform(unsw_test.select_dtypes(['float', 'int']).drop('id', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=1, gamma=1)\n",
    "svm.fit(scaled_train, unsw_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predict = svm.predict(scaled_test)\n",
    "print(classification_report(unsw_test['label'], svm_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron (single layer neural setwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.01, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "           penalty='l2', random_state=0, shuffle=True, tol=0.001,\n",
       "           validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppn = Perceptron(penalty='l2', alpha=0.01)\n",
    "ppn.fit(scaled_train, unsw_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.49      0.64     37000\n",
      "           1       0.70      0.96      0.81     45332\n",
      "\n",
      "    accuracy                           0.75     82332\n",
      "   macro avg       0.80      0.72      0.72     82332\n",
      "weighted avg       0.79      0.75      0.73     82332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppn_predict = ppn.predict(scaled_test)\n",
    "print(classification_report(unsw_test['label'], ppn_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is easy and quick to implement, but makes two (usually) unrealistic assumptions about features: they are independent within classes, and each feature has the same impact on the label assignment.  It is often used to benchmark other more complex algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb =  GaussianNB()\n",
    "nb.fit(unsw_train.select_dtypes float', 'int']).drop('id', axis=1), unsw_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.53      0.62     37000\n",
      "           1       0.69      0.85      0.76     45332\n",
      "\n",
      "    accuracy                           0.71     82332\n",
      "   macro avg       0.72      0.69      0.69     82332\n",
      "weighted avg       0.71      0.71      0.70     82332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predict = nb.predict(unsw_test.select_dtypes(['float', 'int']).drop('id', axis=1))\n",
    "print(classification_report(unsw_test['label'], nb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning: K-means \n",
    "\n",
    "Often we work with data that does not have a label (e.g. attack/normal).  Unsupervised learning can be used to reveal underlying structure in the dataset that informs how the observations seperate in the feature space.  \n",
    "\n",
    "We will use k-means clustering here, which is a common unsupervised learning algorithm: * https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\n",
    "* https://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203\n",
    "\n",
    "The most important hyperparameter in k-means is k, the number of clusters into which you wish to cluster your data.  Because we know our data has a binary label, we will use 2 clusters.  However, in future work we can try k=10, which is the number of attack categories plus a category for normal traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10)\n",
    "pred_y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Components Analysis\n",
    "\n",
    "Based on the correlation heat map, there appears to be substantial correlation.  To improve classifier performance, we are going to use PCA to reduce both the feature dimensionality and correlation. PCA transforms the input data matrx to a matrix of linear independent columns, where the first component maximizes the variability in the original data, the second component is orthogonal to the first component (thus, 0 correlation) while maximizing the variance in the data, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
